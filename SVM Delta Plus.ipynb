{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array,zeros,vstack,repeat,ones,eye,ndarray\n",
    "from cvxopt import *\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So let's talk about the SVM$_\\Delta$+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM$_\\Delta$+ is a varient on the standard SVM and is written about in [this paper](http://www.jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf), where instead of just accepting input $X$, and output $Y$, the classifier also has access to extra privileged information $X*$ at training time, but crucially, not at any point past then. So when we come to making predictions, we'll be making them based solely on data looks a lot like $X$. (Quick note, $X*$ doesn't have to be in the same feature space as $X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, some things we're gonna need. Just gonna blast through these at the top because then they're done and out of the way.\n",
    "\n",
    "We need a data generator as we're going to use toy data (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates 2D data that is largely linearly separable, but overlaps slightly\n",
    "def gen_lin_separable_overlap_data(n):\n",
    "    mean1 = np.array([0, 1])\n",
    "    mean2 = np.array([1, 0])\n",
    "    cov = np.array([[1.5, 1.0], [1.0, 1.5]])\n",
    "    X1 = np.random.multivariate_normal(mean1, cov, (n//2))\n",
    "    y1 = np.ones(len(X1))\n",
    "    X2 = np.random.multivariate_normal(mean2, cov, (n//2))\n",
    "    y2 = np.ones(len(X2)) * -1\n",
    "    return X1, y1, X2, y2\n",
    "\n",
    "# Generates 2D data that is linearly separable - no overlap\n",
    "def gen_lin_separable_data(n):\n",
    "    # generate training data in the 2-d case\n",
    "    mean1 = np.array([0, 2])\n",
    "    mean2 = np.array([2, 0])\n",
    "    cov = np.array([[0.8, 0.6], [0.6, 0.8]])\n",
    "    X1 = np.random.multivariate_normal(mean1, cov, (n//2))\n",
    "    y1 = np.ones(len(X1))\n",
    "    X2 = np.random.multivariate_normal(mean2, cov, (n//2))\n",
    "    y2 = np.ones(len(X2)) * -1\n",
    "    return X1, y1, X2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're also going to need a way of visualising this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_margin(X1_train, X2_train, clf):\n",
    "    def f(x, w, b, c=0):\n",
    "        # given x, return y such that [x,y] in on the line\n",
    "        # w.x + b = c\n",
    "        return (-w[0] * x - b + c) / w[1]\n",
    "\n",
    "    pl.plot(X1_train[:,0], X1_train[:,1], \"ro\", label=\"Class +1\")\n",
    "    pl.plot(X2_train[:,0], X2_train[:,1], \"bo\", label=\"Class -1\")\n",
    "    pl.scatter(clf.support_vectors[:,0], clf.support_vectors[:,1], s=100, c=\"g\")\n",
    "\n",
    "    # w.x + b = 0\n",
    "    a0 = -4; a1 = f(a0, clf.w, clf.b)\n",
    "    b0 = 4; b1 = f(b0, clf.w, clf.b)\n",
    "    pl.plot([a0,b0], [a1,b1], \"k\")\n",
    "\n",
    "    # w.x + b = 1\n",
    "    a0 = -4; a1 = f(a0, clf.w, clf.b, 1)\n",
    "    b0 = 4; b1 = f(b0, clf.w, clf.b, 1)\n",
    "    pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "    # w.x + b = -1\n",
    "    a0 = -4; a1 = f(a0, clf.w, clf.b, -1)\n",
    "    b0 = 4; b1 = f(b0, clf.w, clf.b, -1)\n",
    "    pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "    pl.xlabel('x1')\n",
    "    pl.ylabel('x2')\n",
    "\n",
    "    pl.legend(numpoints=1)\n",
    "\n",
    "    pl.axis(\"tight\")\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore this bit, it's so we can have different kernels on our data... It shouldn't make a difference for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __call__(self, a, b):\n",
    "        x = np.array(a)\n",
    "        y = np.array(b)\n",
    "        y = np.transpose(y)\n",
    "        return np.dot(x, y)\n",
    "\n",
    "class Polynomial():\n",
    "    def __call__(self, a, b, p=3):\n",
    "        x = np.array(a)\n",
    "        y = np.array(b)\n",
    "        y = np.transpose(y)\n",
    "        return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "class Gaussian():\n",
    "    def __call__(self, a, b, sigma=5.0):\n",
    "        x = np.array(a)\n",
    "        y = np.array(b)\n",
    "        y = np.transpose(y)\n",
    "        return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're going to create a \"problem\" class. This is just somewhere where we keep the data that we're training the SVM on, but it also holds some hyperparameters that we might need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class svm_problem():\n",
    "    def __init__(self, C=1.0, gamma=1.0, delta=1.0, kernel=Linear()):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def set_variables(self, X, Xstar, Y):\n",
    "        if(isinstance(X, ndarray)):\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = array(X)\n",
    "        if(isinstance(Xstar, ndarray)):\n",
    "            self.Xstar = Xstar\n",
    "        else:\n",
    "            self.Xstar = array(Xstar)\n",
    "        if(isinstance(Y, ndarray)):\n",
    "            self.Y = Y\n",
    "        else:\n",
    "            self.Y = array(Y)\n",
    "        self.num = len(self.X)\n",
    "        self.dimensions = len(self.X[0])\n",
    "        self.xi_xj = self.gram_matrix(self.X, self.X)\n",
    "        self.xstari_xstarj = self.gram_matrix(self.Xstar, self.Xstar)\n",
    "        self.yi_yj = self.gram_matrix(self.Y, self.Y)\n",
    "        self.xi_star = zeros(self.num)\n",
    "        self.xi = zeros(self.num)\n",
    "        self.zeta = zeros(self.num)\n",
    "\n",
    "    def gram_matrix(self, X1, X2):\n",
    "        K = zeros((len(X1), len(X1)))\n",
    "        for i in range(len(X1)):\n",
    "            for j in range(len(X1)):\n",
    "                K[i,j] = self.kernel(X1[i], X2[j])\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, so what is it that we're solving?\n",
    "\n",
    "The regular SVM looks like this. We want to\n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\textbf{w},b,\\xi} \\quad & \\frac{1}{2}||\\textbf{w}||^2 + C\\sum_{i=1}^{\\ell}\\xi_i\\\\\n",
    "\\textrm{subject to} & \\quad y_i(\\textbf{w} \\cdot \\textbf{x}_i+b)\\geq 1 - \\xi_i \\\\\n",
    "\\textrm{and} & \\quad \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "Meaning we want to make the maergin as wide as possible in our classifier, but we're going to be penalised for every breach of that margin. So, breaches are allowed, but they cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out this is actually quite hard to solve as the conditions are quite complex. If we change this to it's dual \n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha} \\sum_{i=1}^{\\ell}\\alpha_i - \\frac{1}{2} & \\sum_{i,j=1}^{\\ell}\\alpha_i \\alpha_j y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j) \\\\\n",
    "\\textrm{subject to} & \\quad 0 < \\alpha_i < C \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "We get a convex optimisation problem that can be solved using such convex optimisation tools as CVXOPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVXOPT solves problems in the following format\n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{x} \\quad & \\frac{1}{2}x^TPx+q^Tx \\\\\n",
    "\\textrm{subject to} \\quad & Gx \\leq h \\\\\n",
    "\\textrm{and} \\quad & Ax = b\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which as you can see is sort of the same shape as the dual form of the SVM problem. But instead of minimising over $x$ we want to maximise over $/alpha$. Doing this is easy enough. We just have to do a bit of rearranging, giving us\n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha} & \\quad \\frac{1}{2} \\sum_{i,j=1}^{\\ell}\\alpha_i (y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j)) \\alpha_j - \\sum_{i=1}^{\\ell}\\alpha_i \\\\\n",
    "\\textrm{subject to} & \\quad -\\alpha_i < 0 \\\\\n",
    "\\textrm{and} & \\quad \\alpha_i < C \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form P we make a Gram Matrix of $Y$ and a second of $X$ then multiply them together \n",
    "$$\\begin{equation}\n",
    "P = \n",
    "\\begin{pmatrix} y_1y_1(\\textbf{x}_1\\textbf{x}_1) & \\cdots & y_iy_1(\\textbf{x}_i\\textbf{x}_1)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "y_1y_j(\\textbf{x}_1\\textbf{x}_j) & \\cdots & y_iy_j(\\textbf{x}_i\\textbf{x}_j)\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q is the negative identity matrix $$\\begin{equation}\n",
    "q = \n",
    "\\begin{pmatrix} -1 & 0 & \\cdots & 0 \\\\\n",
    "0 & -1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & -1\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G and h are slightly more tricky as we've got two less than statements, but let's break them down and talk about G1 and h1 being $0\\leq\\alpha_i$ and G2 and h2 being $\\alpha_i\\leq C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G1 = \n",
    "\\begin{pmatrix} -1 & 0 & \\cdots & 0 \\\\\n",
    "0 & -1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & -1\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h1 = \n",
    "\\begin{pmatrix} 0_1 \\\\\n",
    "0_2 \\\\\n",
    "\\vdots \\\\\n",
    "0_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "\n",
    "$-1\\times\\alpha_1\\leq 0 \\cdots -1\\times\\alpha_n\\leq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G2 = \n",
    "\\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & 1\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h2 = \n",
    "\\begin{pmatrix} C_1 \\\\\n",
    "C_2 \\\\\n",
    "\\vdots \\\\\n",
    "C_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "C is a constant, so $C_n$ means the $n^{th}$ repitition of C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we stack them so that $\\begin{equation}\n",
    "G = \n",
    "\\begin{pmatrix} G1 \\\\\n",
    "G2\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "and $\\begin{equation}\n",
    "h = \n",
    "\\begin{pmatrix} h1 \\\\\n",
    "h2\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$ Don't think this is some clever notation. I literally mean G1 stacked on top of G2. Same with h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "A = \n",
    "\\begin{pmatrix} y_1 & y_2 & \\cdots & y_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$ $\\begin{equation}\n",
    "b = \n",
    "\\begin{pmatrix} 0\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that A is a vector, not a column matrix. This is because we want the sum of $\\alpha y$ to be 0, not element-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets go ahead and put this in a solver. This might be a bit dull, but first we're going to generate some data. $X$ is going to have some overlap, but the privileged data will be separable. As at the moment we're just looking at the regular SVM, we'll ignore the $X*$ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = gen_lin_separable_overlap_data(20)\n",
    "X = np.vstack((x1,x2))\n",
    "Y = np.hstack((y1,y2))\n",
    "x3,y1,x4,y2 = gen_lin_separable_data(20)\n",
    "Xstar = np.vstack((x3,x4))\n",
    "\n",
    "prob = svm_problem(C=1.0)\n",
    "prob.set_variables(X, Xstar, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = prob.X\n",
    "y = prob.Y\n",
    "C = prob.C\n",
    "\n",
    "NUM = x.shape[0]\n",
    "DIM = x.shape[1]\n",
    "\n",
    "Ky = prob.yi_yj\n",
    "Kx = prob.xi_xj\n",
    "K = Ky*Kx\n",
    "P = matrix(K)\n",
    "q = matrix(-np.ones((NUM, 1)))\n",
    "G1 = -np.eye(NUM)\n",
    "G2 = np.eye(NUM)\n",
    "G = np.vstack((G1, G2))\n",
    "G = matrix(G)\n",
    "h1 = np.zeros(NUM).reshape(-1,1)\n",
    "h2 = np.repeat(C, NUM).reshape(-1,1)\n",
    "h = np.vstack((h1, h2))\n",
    "h = matrix(h)\n",
    "A = matrix(y.reshape(1, -1))\n",
    "b = matrix(np.zeros(1))\n",
    "solvers.options['show_progress'] = False\n",
    "sol = solvers.qp(P, q, G, h, A, b)\n",
    "alphas = np.array(sol['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print out alphas, you should get an array of values that are proactiaclly 0, except for the supprt vectors. They're slightly bigger than 0. Depending on the hyperparameters, there should be very few of them.\n",
    "\n",
    "So having the alphas is great, but we want to extract weights and bias to make a prediction... So lets do that now.\n",
    "\n",
    "$$\\textbf{w} = \\sum_{i=1}^{L}\\alpha_iy_i\\textbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.sum(alphas * y[:, None] * x, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias is a bit trickier. We need to get the set of support vectors ($S$) that match $0<\\alpha_i\\leqC$ then solve\n",
    "\n",
    "$$b = \\frac{\\sum_{s=1}^{S}(y_s-\\sum_{m=1}^{S}\\alpha_my_m(\\textbf{x}_m\\cdot\\textbf{x}_s))}{\\textrm{Number of S}}$$\n",
    "\n",
    "As you might imagine, this is a bit of a faff, but below does the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bacond1 = (alphas > 1e-5)\n",
    "bdcond2 = (alphas < C)\n",
    "\n",
    "bcond = np.array([a and b for a, b in zip(bacond1, bdcond2)]).flatten()\n",
    "\n",
    "yS = y[bcond]\n",
    "xS = x[bcond]\n",
    "aS = alphas[bcond]\n",
    "\n",
    "\n",
    "sumTotal = 0\n",
    "for s in range(len(yS)):\n",
    "    innerTotal = 0\n",
    "    for m in range(len(yS)):\n",
    "        am = aS[m]\n",
    "        ym = yS[m]\n",
    "        xm_xs = prob.kernel(xS[m], xS[s])\n",
    "        innerTotal += am*ym*xm_xs\n",
    "    sumTotal += yS[s] - innerTotal\n",
    "\n",
    "bias = sumTotal/len(yS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put this in a a dull, but useful class called classifier, just to keep everything neat and out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        self.alphas = []\n",
    "        self.support_vectors = []\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.sign(np.dot(self.w,x)+self.b)\n",
    "    \n",
    "    def f_star(self, x, y): # This won't make sense now, but we come back to it later\n",
    "        return y*(np.dot(self.w,x)+self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_clf = classifier()\n",
    "svm_clf.w = w\n",
    "svm_clf.b = bias\n",
    "svm_clf.alphas = alphas\n",
    "svm_clf.support_vectors = prob.X[bacond1.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it.... a trained SVM. Shall we take a look at it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_margin(prob.X[y==1], prob.X[y==-1], svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is all a bit sexy. Go back to the problem definition and play with the C value. the bigge then number (like 10?), the harder the margin, the lower (say, 0.01), the softer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT WHAT DOES THIS HAVE TO DO WITH SVM$_\\Delta$+? Everything. There are two versions, a simplified approach and a non-simplified approach. Let's break them down. But first I'm just going to put the above in a class just so that it's eaiser if we want to use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def train(self, x, prob : svm_problem):\n",
    "        x = x\n",
    "        y = prob.Y\n",
    "        C = prob.C\n",
    "\n",
    "        NUM = x.shape[0]\n",
    "        DIM = x.shape[1]\n",
    "\n",
    "        K = y[:, None] * x # Yeah, this is a bit different so that it can work on x and x*\n",
    "        K = np.dot(K, K.T)\n",
    "        P = matrix(K)\n",
    "        q = matrix(-np.ones((NUM, 1)))\n",
    "        G1 = -np.eye(NUM)\n",
    "        G2 = np.eye(NUM)\n",
    "        G = np.vstack((G1, G2))\n",
    "        G = matrix(G)\n",
    "        h1 = np.zeros(NUM).reshape(-1,1)\n",
    "        h2 = np.repeat(C, NUM).reshape(-1,1)\n",
    "        h = np.vstack((h1, h2))\n",
    "        h = matrix(h)\n",
    "        A = matrix(y.reshape(1, -1))\n",
    "        b = matrix(np.zeros(1))\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.array(sol['x'])\n",
    "        w = np.sum(alphas * y[:, None] * x, axis = 0)\n",
    "        bacond1 = (alphas > 1e-5)\n",
    "        bdcond2 = (alphas < C)\n",
    "        bcond = np.array([a and b for a, b in zip(bacond1, bdcond2)]).flatten()\n",
    "        yS = y[bcond]\n",
    "        xS = x[bcond]\n",
    "        aS = alphas[bcond]\n",
    "        sumTotal = 0\n",
    "        for s in range(len(yS)):\n",
    "            innerTotal = 0\n",
    "            for m in range(len(yS)):\n",
    "                am = aS[m]\n",
    "                ym = yS[m]\n",
    "                xm_xs = prob.kernel(xS[m], xS[s])\n",
    "                innerTotal += am*ym*xm_xs\n",
    "            sumTotal += yS[s] - innerTotal\n",
    "        bias = sumTotal/len(yS)\n",
    "        clf = classifier()\n",
    "        clf.w = w\n",
    "        clf.b = bias[0]\n",
    "        clf.alphas = alphas\n",
    "        clf.support_vectors = x[bacond1.flatten()]\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM$_\\Delta$+ - The simplified approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplified approach uses the SVM we've already written. If you want to see this on the paper we're following, it's [page 2034](http://www.jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf \"Vapnik paper\"). Firstly we learn an SVM in the privileged space $X*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVM()\n",
    "xStar_clf = svm.train(prob.Xstar, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this to work out a set of slack values in the privileged space $\\xi^* = [1-f(x^*)-b^*]_+$. This means we choose the highest value of 0 or the predicted value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xi_star = np.zeros(prob.num)\n",
    "for i in range(prob.num):\n",
    "    output = (1-xStar_clf.f_star(prob.Xstar[i], prob.Y[i]) - xStar_clf.b)\n",
    "    xi_star[i] = max(0, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we were to look at the $X*$ classifier on a graph, we should see that there aren't really any breaches as we've used a linearly sepearble set of data for $X*$, consequently, the values of $\\xi^*$ should also all be almost 0 (except the ones that breach the margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_margin(prob.Xstar[y==1], prob.Xstar[y==-1], xStar_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xi_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then basically train another SVM. This time though, instead of using exactly the same model as before, we have to do a different one as there are different constraints. Let's look at them. \n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha} \\sum_{i=1}^{\\ell}\\alpha_i - \\frac{1}{2} & \\sum_{i,j=1}^{\\ell}\\alpha_i \\alpha_j y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j) \\\\\n",
    "\\textrm{subject to} & \\quad \\sum_{i=1}^{\\ell}\\alpha_i\\xi^*_i \\leq C\\sum_{i=1}^{\\ell}\\xi^*_i \\\\\n",
    "\\textrm{and} & \\quad 0 < \\alpha_i < (1+\\Delta)C \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's break it down. The main optimisation problem is the same as a regular SVM, so we can use the same P and q as before. The equality constraint is the same, so we have the same A and b. We just need a new G and h. Let's break it down. G1 and h1 give $-\\alpha\\leq0$, G2 and h2 $\\alpha\\leq(1+\\Delta)C$ and G3 and h3 being $\\sum_{i=1}^{\\ell}\\alpha_i\\xi^*_i \\leq C\\sum_{i=1}^{\\ell}\\xi^*_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G1 = \n",
    "\\begin{pmatrix} -1 & 0 & \\cdots & 0 \\\\\n",
    "0 & -1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & -1_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h1 = \n",
    "\\begin{pmatrix} 0_1 \\\\\n",
    "0_2 \\\\\n",
    "\\vdots \\\\\n",
    "0_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G2 = \n",
    "\\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & 1_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h2 = \n",
    "\\begin{pmatrix} (1+\\Delta)C_1 \\\\\n",
    "(1+\\Delta)C_2 \\\\\n",
    "\\vdots \\\\\n",
    "(1+\\Delta)C_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G3 = \n",
    "\\begin{pmatrix} \\xi_1^* & \\xi_2^* & \\cdots & \\xi_n^*\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$ $\\begin{equation}\n",
    "h3 = \n",
    "\\begin{pmatrix} C\\sum_{i=1}^{\\ell}\\xi_i^*\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G = \n",
    "\\begin{pmatrix} G1 \\\\\n",
    "G2 \\\\\n",
    "G3\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "and $\\begin{equation}\n",
    "h = \n",
    "\\begin{pmatrix} h1 \\\\\n",
    "h2 \\\\\n",
    "h3\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = prob.X\n",
    "y = prob.Y\n",
    "C = prob.C\n",
    "\n",
    "NUM = x.shape[0]\n",
    "DIM = x.shape[1]\n",
    "\n",
    "Ky = prob.yi_yj\n",
    "Kx = prob.xi_xj\n",
    "K = Ky*Kx\n",
    "P = matrix(K)\n",
    "q = matrix(-np.ones((NUM, 1)))\n",
    "G1 = -np.eye(NUM)\n",
    "G2 = np.eye(NUM)\n",
    "G3 = xi_star.reshape(1,-1)\n",
    "G = np.vstack((G1, G2))\n",
    "G = np.vstack((G, G3))\n",
    "G = matrix(G)\n",
    "h1 = np.zeros(NUM).reshape(-1,1)\n",
    "h2 = np.repeat(C, NUM).reshape(-1,1)\n",
    "h3 = sum(xi_star)*C\n",
    "h = np.vstack((h1, h2))\n",
    "h = np.vstack((h, h3))\n",
    "h = matrix(h)\n",
    "A = matrix(y.reshape(1, -1))\n",
    "b = matrix(np.zeros(1))\n",
    "solvers.options['show_progress'] = False\n",
    "sol = solvers.qp(P, q, G, h, A, b)\n",
    "alphas = np.array(sol['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.sum(alphas * y[:, None] * x, axis = 0)\n",
    "\n",
    "bacond1 = (alphas > 1e-5)\n",
    "bdcond2 = (alphas < C)\n",
    "bcond = np.array([a and b for a, b in zip(bacond1, bdcond2)]).flatten()\n",
    "\n",
    "yS = y[bcond]\n",
    "xS = x[bcond]\n",
    "aS = alphas[bcond]\n",
    "\n",
    "sumTotal = 0\n",
    "for s in range(len(yS)):\n",
    "    innerTotal = 0\n",
    "    for m in range(len(yS)):\n",
    "        am = aS[m]\n",
    "        ym = yS[m]\n",
    "        xm_xs = prob.kernel(xS[m], xS[s])\n",
    "        innerTotal += am*ym*xm_xs\n",
    "    sumTotal += yS[s] - innerTotal\n",
    "\n",
    "bias = sumTotal/len(yS)\n",
    "\n",
    "svmdpsa_clf = classifier() # svmdpsa means svm delta plus simplified approach\n",
    "svmdpsa_clf.w = w\n",
    "svmdpsa_clf.b = bias\n",
    "svmdpsa_clf.alphas = alphas\n",
    "svmdpsa_clf.support_vectors = prob.X[bacond1.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out! The old classifier is on top and the new one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_margin(prob.X[y==1], prob.X[y==-1], svm_clf), plot_margin(prob.X[y==1], prob.X[y==-1], svmdpsa_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait... after all that, they're the same?!?! What gives? Yeah... I'd have loved it if if was more impressive. But the only additional term is $\\sum_{i=1}^{\\ell}\\alpha_i\\xi_i^*\\leq C\\sum_{i=1}^{\\ell}\\xi_i^*$ and if your $\\xi^*$, the margin violation in the privileged space, is 0. Well, it's not gonna make any difference. Let alone the fact that most $\\alpha$'s are 0. We're looking for support vectors (non-zero $\\alpha$'s) in $X$ space, that the data for them in the corresponding priviliged space $X^*$ is in breach of the margin. And given this toy data, that's not happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM$_\\Delta$+ - The not so simple approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the simplified approach wax a bit of a let down on the toy data. We might come back to it with some real data and see if it improves, but until then, let's look at the more complicated version and see if it manages to do any better. Here it is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\min_{\\textbf{w},b,\\textbf{w}^*,b^*} \\quad \\frac{1}{2}(||\\textbf{w}||^2 + \\gamma ||\\textbf{w}^*||^2) & + C\\sum_{i=1}^{\\ell} [y_i((\\textbf{w}^* \\cdot \\textbf{x}^*_i) + b^*) + \\zeta_i] + \\Delta C \\sum_{i=1}^{\\ell}\\zeta_i \\\\\n",
    "\\textrm{subject to} & \\quad y_i(\\textbf{w} \\cdot \\textbf{x}_i+b)\\geq 1 - y_i((\\textbf{w}^* \\cdot \\textbf{x}^*_i) + b^*)-\\zeta_i \\\\\n",
    "\\textrm{and} & \\quad y_i((\\textbf{w}^* \\cdot \\textbf{x}^*_i) + b^*) + \\zeta_i \\geq 0 \\\\\n",
    "\\textrm{and} & \\quad \\zeta_i \\geq 0 \n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crumbs, that looks complicated. But it's similar in form to the SVM. We have a term that we want to minimise subject to a few things. Let's do what we did with the SVM and get the dual form and see if we can follow the steps we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha, (C-\\beta)} \\sum_{i=1}^{\\ell}\\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{\\ell}\\alpha_i \\alpha_j y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j) - & \\frac{1}{2\\gamma}\\sum_{i, j = 1}^{\\ell}y_i y_j(C-\\beta_i - \\alpha_i)(C-\\beta_j - \\alpha_j)(\\textbf{x}_i^* \\cdot \\textbf{x}_j^*) \\\\\n",
    "\\textrm{subject to} & \\quad \\sum_{i=1}^{\\ell} y_i \\alpha_i = 0 \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{\\ell} y_i C-\\beta_i = 0 \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq C-\\beta_i \\leq C \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq \\alpha_i \\leq C-\\beta_i +\\Delta C\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's neaten it up a bit and say $\\beta-C = \\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha, \\delta} \\sum_{i=1}^{\\ell}\\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{\\ell}\\alpha_i \\alpha_j y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j) - & \\frac{1}{2\\gamma}\\sum_{i, j = 1}^{\\ell}y_i y_j(\\delta_i - \\alpha_i)(\\delta_j - \\alpha_j)(\\textbf{x}_i^* \\cdot \\textbf{x}_j^*) \\\\\n",
    "\\textrm{subject to} & \\quad \\sum_{i=1}^{\\ell} y_i \\alpha_i = 0 \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{\\ell} y_i \\delta_i = 0 \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq \\delta_i \\leq C \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq \\alpha_i \\leq \\delta_i +\\Delta C\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at the constraints, goven that we've done this kind of thing before, they shouldn't be too much of a problem. Yes, there's now an extra variable, $\\delta$, but we can deal with that. The more scary looking one is the top line. Unfotunately I'm not an expert in just wizardly turning that into one matrix. However, [this paper](http://www.cse.psu.edu/~zbc102/files/svm_plus_technical_report_15.pdf) solves a problem that looks very similar.\n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha, \\delta} \\sum_{i=1}^{\\ell}\\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{\\ell}\\alpha_i \\alpha_j y_i y_j (\\textbf{x}_i \\cdot \\textbf{x}_j) - & \\frac{\\gamma}{2}\\sum_{i, j = 1}^{\\ell}y_i y_j(\\alpha_i- \\delta_i)(\\alpha_j - \\delta_j)(\\textbf{x}_i^* \\cdot \\textbf{x}_j^*) \\\\\n",
    "\\textrm{subject to} & \\quad \\sum_{i=1}^{\\ell} y_i \\alpha_i = 0 \\\\\n",
    "\\textrm{and} & \\quad \\sum_{i=1}^{\\ell} y_i \\delta_i = 0 \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq \\delta_i \\leq C_i \\\\\n",
    "\\textrm{and} & \\quad 0 \\leq \\alpha_i \\leq \\kappa C_i\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the differences \n",
    "1. The placing of $\\gamma$, well that's no biggie, it will still have the same effect. \n",
    "2. We talk about $(\\alpha_i - \\delta_i)$, the other paper solves $(\\delta_i - \\alpha_i)$. Now this could be a bit of a problem, we'll talk about it in a bit.\n",
    "3. They talk about $\\kappa C$, we're looking at $\\delta_i + \\Delta C$. That's easy enough, we can just change a contstraint a little bit.\n",
    "4. They talk about $C_i$. This is a bit confusing as $C$ is a constant. I'm hoping this is a typo and nothing more profound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the constraints out of the way, because we can do them easily. First off, the inequalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G1 and h1 $-\\delta_i \\leq 0$\n",
    "\n",
    "G2 and h2 $\\delta_i \\leq C$\n",
    "\n",
    "G3 and h3 $-\\alpha_i \\leq 0$\n",
    "\n",
    "G4 and h4 $\\alpha_i \\leq \\delta_i + \\Delta C$ which we'll jig about to be $\\alpha_i - \\delta_i \\leq  \\Delta C$\n",
    "\n",
    "The thing to remember is that these are being multiplied by two variables, as such we need to remember that we will be multiplying $G$ by $\\begin{equation}\n",
    "\\begin{pmatrix} \\alpha \\\\\n",
    "\\delta\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G1 = \n",
    "\\begin{pmatrix} 0 & 0 & \\cdots & 0_{1,n} & -1_{1,n+1} & 0 & \\cdots & 0_{1,2n} \\\\\n",
    "0 & 0 & \\cdots & 0_{2,n} & 0_{2,n+1} & -1 & \\cdots & 0_{2,2n} \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots & \\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0_{n,1} & \\cdots & 0 & 0_{n,n} & 0_{n, n+1} & \\cdots & 0 & -1_{n,2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h1 = \n",
    "\\begin{pmatrix} 0_1 \\\\\n",
    "0_2 \\\\\n",
    "\\vdots \\\\\n",
    "0_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G2 = \n",
    "\\begin{pmatrix} 0 & 0 & \\cdots & 0_{1,n} & 1_{1,n+1} & 0 & \\cdots & 0_{1,2n} \\\\\n",
    "0 & 0 & \\cdots & 0_{2,n} & 0_{2,n+1} & 1 & \\cdots & 0_{2,2n} \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots & \\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0_{n,1} & \\cdots & 0 & 0_{n,n} & 0_{n, n+1} & \\cdots & 0 & 1_{n,2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h2 = \n",
    "\\begin{pmatrix} C_1 \\\\\n",
    "C_2 \\\\\n",
    "\\vdots \\\\\n",
    "C_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G3 = \n",
    "\\begin{pmatrix} -1_{1,1} & 0 & \\cdots & 0_{1,n} & 0_{1,n+1} & 0 & \\cdots & 0_{1,2n} \\\\\n",
    "0_{2,1} & -1 & \\cdots & 0_{2,n} & 0_{2,n+1} & 0 & \\cdots & 0_{2,2n} \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots & \\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0_{n, 1} & \\cdots & 0 & -1_{n,n} & 0_{n,n+1} & \\cdots & 0 & 0_{n,2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h3 = \n",
    "\\begin{pmatrix} 0_1 \\\\\n",
    "0_2 \\\\\n",
    "\\vdots \\\\\n",
    "0_n\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "G4 = \n",
    "\\begin{pmatrix} 1_{1,1} & 0 & \\cdots & 0_{1,n} & -1_{1,n+1} & 0 & \\cdots & 0_{1,2n} \\\\\n",
    "0_{2,1} & 1 & \\cdots & 0_{2,n} & 0_{2,n+1} & -1 & \\cdots & 0_{2,2n} \\\\\n",
    "\\vdots & \\cdots & \\ddots & \\vdots & \\vdots & \\cdots & \\ddots & \\vdots \\\\\n",
    "0_{n, 1} & \\cdots & 0 & 1_{n,n} & 0_{n,n+1} & \\cdots & 0 & -1_{n,2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "$\\begin{equation}\n",
    "h4 = \n",
    "\\begin{pmatrix} \\Delta C \\\\\n",
    "\\Delta C \\\\\n",
    "\\vdots \\\\\n",
    "\\Delta C\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as before $\\begin{equation}\n",
    "G = \n",
    "\\begin{pmatrix} G1 \\\\\n",
    "G2 \\\\\n",
    "G3 \\\\\n",
    "G4\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "and $\\begin{equation}\n",
    "h = \n",
    "\\begin{pmatrix} h1 \\\\\n",
    "h2 \\\\\n",
    "h3 \\\\\n",
    "h4\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the equalities!\n",
    "\n",
    "A1 = $\\sum_{i=1}^{\\ell}y_i\\alpha_i$ b1 = $0$\n",
    "A2 = $\\sum_{i=1}^{\\ell}y_i\\delta_i$ b2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "A1 = \n",
    "\\begin{pmatrix} y_1 & y_2 & \\cdots & y_n & 0_{n+1} & 0_{n+2} & \\cdots & 0_{2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$ $\\begin{equation}\n",
    "b1 = \n",
    "\\begin{pmatrix} 0\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "A2 = \n",
    "\\begin{pmatrix} 0_{1} & 0_{2} & \\cdots & 0_{n} & y_{n+1} & y_{n+2} & \\cdots & y_{2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$ $\\begin{equation}\n",
    "b2 = \n",
    "\\begin{pmatrix} 0\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then stack as with inequalities. \n",
    "$\\begin{equation}\n",
    "A = \n",
    "\\begin{pmatrix} A1 \\\\\n",
    "A2 \n",
    "\\end{pmatrix} \n",
    "\\end{equation}$\n",
    "and $\\begin{equation}\n",
    "b = \n",
    "\\begin{pmatrix} b1 \\\\\n",
    "b2 \n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, you could've probably done them yourself. But then I wouldn't get to put off this part. In the paper that solves a similar problem, they call their $P$ and $q$, $H$ and $f$. Fine. The $f$ is easy, we're doing a sum over all the negative $\\alpha$'s, so $\\begin{equation}\n",
    "f = q = \n",
    "\\begin{pmatrix} -1_1 & -1_2 & \\cdots & -1_n & 0_{n+1} & \\cdots & 0_{2n}\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper gives $\\begin{equation}\n",
    "H = \n",
    "\\begin{pmatrix} (\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j & -\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j \\\\\n",
    "-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j & +\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j\n",
    "\\end{pmatrix} \n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I've got a bit more time I'll do a proper expansion of H, just know that it's a 2n by 2n matrix. The bit that's confusing me is that we need to go from the optimisation problem to H. Maybe we can work it backwards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "\\begin{pmatrix} \\alpha & \\delta \\end{pmatrix} \\times\n",
    "\\begin{pmatrix} (\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j & -\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j \\\\\n",
    "-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j & +\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j\n",
    "\\end{pmatrix} \\times \n",
    "\\begin{pmatrix} \\alpha \\\\\n",
    "\\delta\n",
    "\\end{pmatrix}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix} (\\alpha((\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j))+(\\delta(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j)) &\n",
    "(\\alpha(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j))+ \n",
    "(\\delta(\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j))\n",
    "\\end{pmatrix} \\times \n",
    "\\begin{pmatrix} \\alpha \\\\\n",
    "\\delta\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "\\alpha(\\alpha((\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j) +\n",
    "(\\delta(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j))) & + &\n",
    "\\delta(\\alpha(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j)+ \n",
    "(\\delta(\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j)))\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "\\alpha\\alpha((\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j) +\n",
    "\\alpha\\delta(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j) & + &\n",
    "\\delta\\alpha(-\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j)+ \n",
    "\\delta\\delta(\\gamma(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j)\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "\\alpha\\alpha(\\textbf{x}_i,\\textbf{x}_j)y_iy_j + \\gamma\\alpha\\alpha(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j\n",
    "-\\gamma\\alpha\\delta(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j\n",
    "-\\gamma\\delta\\alpha(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j+ \n",
    "\\gamma\\delta\\delta(\\textbf{x}^*_i,\\textbf{x}^*_j)y_iy_j\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "\\alpha\\alpha y_iy_j(\\textbf{x}_i,\\textbf{x}_j) + \\gamma y_iy_j(\\textbf{x}^*_i,\\textbf{x}^*_j)\\alpha\\alpha\n",
    "-\\gamma y_iy_j(\\textbf{x}^*_i,\\textbf{x}^*_j)\\alpha\\delta\n",
    "-\\gamma y_iy_j(\\textbf{x}^*_i,\\textbf{x}^*_j)\\alpha\\delta+ \n",
    "\\gamma y_iy_j(\\textbf{x}^*_i,\\textbf{x}^*_j)\\delta\\delta\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{pmatrix}\n",
    "\\alpha\\alpha y_iy_j(\\textbf{x}_i,\\textbf{x}_j) + \\gamma y_iy_j(\\textbf{x}^*_i,\\textbf{x}^*_j)\n",
    "(\\alpha\\alpha-\\alpha\\delta-\\alpha\\delta+\\delta\\delta)\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you've followed those steps. I'll try to come back and annotate them a bit more, but now we've got to an interesting part. $(\\alpha\\alpha - \\alpha\\delta - \\alpha\\delta + \\delta\\delta)$. If we factorise this, we get... $(\\alpha-\\delta)(\\alpha-\\delta)$. Which is great. But we can also get $(\\delta-\\alpha)(\\delta-\\alpha)$. Which is really sweet. The same $H$ or $P$ matrix works for both the problems. Awesome! So let's populate this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = prob.kernel\n",
    "C = prob.C\n",
    "\n",
    "L = prob.num\n",
    "M = prob.dimensions\n",
    "\n",
    "x = prob.X\n",
    "y = prob.Y\n",
    "\n",
    "gamma = prob.gamma\n",
    "delta = prob.delta\n",
    "\n",
    "H11 = (prob.xi_xj * prob.yi_yj) + gamma*(prob.xstari_xstarj * prob.yi_yj)\n",
    "H12 = -gamma*(prob.xstari_xstarj * prob.yi_yj)\n",
    "H22 = gamma*(prob.xstari_xstarj * prob.yi_yj)\n",
    "H1 = np.hstack((H11, H12))\n",
    "H2 = np.hstack((H12, H22))\n",
    "H = np.vstack((H1, H2))\n",
    "\n",
    "f = np.hstack((np.repeat(-1, L),np.zeros(L)))\n",
    "\n",
    "positiveEye = np.eye(L, dtype='d')\n",
    "negativeEye = -np.eye(L, dtype='d')\n",
    "zeros = np.zeros((L, L))\n",
    "g1 = np.hstack((zeros, negativeEye))\n",
    "g2 = np.hstack((zeros, positiveEye))\n",
    "g3 = np.hstack((negativeEye, zeros))\n",
    "g4 = np.hstack((positiveEye, negativeEye))\n",
    "\n",
    "G = np.vstack((g1,g2))\n",
    "G = np.vstack((G,g3))\n",
    "G = np.vstack((G,g4))\n",
    "\n",
    "h1 = np.zeros(((L),1))\n",
    "h2 = np.repeat(C, (L)).reshape(-1,1)\n",
    "h2 = np.vstack((h1, h2))\n",
    "h3 = np.vstack((h2, h1))\n",
    "h4 = np.repeat((delta*C), L).reshape(-1,1)\n",
    "h = np.vstack((h3, h4))\n",
    "\n",
    "Aeq1 = np.hstack((prob.Y, np.zeros(L)))\n",
    "Aeq2 = np.hstack((np.zeros(L), prob.Y))\n",
    "Aeq = np.vstack((Aeq1, Aeq2))\n",
    "\n",
    "beq = np.zeros(2)\n",
    "beq = beq.reshape(-1,1)\n",
    "\n",
    "P = matrix(H, tc='d')\n",
    "q = matrix(f, tc='d')\n",
    "G = matrix(G, tc='d')\n",
    "h = matrix(h, tc='d')\n",
    "A = matrix(Aeq, tc='d')\n",
    "b = matrix(beq, tc='d')\n",
    "\n",
    "solvers.options['show_progress'] = False\n",
    "sol = solvers.qp(P, q, G, h, A, b)\n",
    "alphasAndDeltas = np.array(sol['x'])\n",
    "alphas = alphasAndDeltas[:L]\n",
    "deltas = alphasAndDeltas[L:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we get $\\textbf{w}$ in the same way we always have done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.sum(alphas * y[:, None] * x, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias however, is slightly different. Instead of $b = \\frac{\\sum_{s=1}^{S}(y_s-\\sum_{m=1}^{S}\\alpha_my_m(\\textbf{x}_m\\cdot\\textbf{x}_s))}{\\textrm{Number of S}}$ we have $b=1-y_k[\\sum_{i=1}^{\\ell}\\alpha_i(\\textbf{x}_i,\\textbf{x}_k)]$ where $x_i\\neq 0$, $\\delta_k \\neq C$ and $\\alpha_k \\neq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bacond = (alphas > 1e-5)\n",
    "bdcond = (deltas < C)\n",
    "bxcond = (x != 0)\n",
    "\n",
    "bxcond2 = list(range(0, L))\n",
    "index = 0\n",
    "for dataPoint in bxcond:\n",
    "    for point in dataPoint:\n",
    "        if point == False:\n",
    "            bxcond2[index] = False\n",
    "            break\n",
    "        bxcond2[index] = True\n",
    "    index += 1\n",
    "\n",
    "bcond = np.array([a and b for a, b in zip(bacond, bdcond)]).flatten()\n",
    "bcond = np.array([a and b for a, b in zip(bcond, bxcond2)]).flatten()\n",
    "yK = y[bcond]\n",
    "xK = x[bcond]\n",
    "\n",
    "b = []\n",
    "for k in range(len(xK)):\n",
    "    b.append(1-yK[k]*np.dot(w, xK[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I'm not entirely happy. The there are multiple posints where the $x_i\\neq 0$, $\\delta_k \\neq C$ and $\\alpha_k \\neq 0$ contraints are valid. So, I'm going to just take the average (mean) point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = (1- (sum(b) / len(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmdp_clf = classifier() # svmdpsa means svm delta plus simplified approach\n",
    "svmdp_clf.w = w\n",
    "svmdp_clf.b = bias\n",
    "svmdp_clf.alphas = alphas\n",
    "svmdp_clf.support_vectors = prob.X[bacond.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like shall we? The originl classifier is on top, the SVM$_\\Delta$+ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_margin(prob.X[y==1], prob.X[y==-1], svm_clf), plot_margin(prob.X[y==1], prob.X[y==-1], svmdp_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoah!!!! What's happened? Well, the data is generated randomly, so what you're seeing isn't the same as what I am. But you should see that the support vectors (the ones with green circles around them) are a little different and the boundary's moved a bit. Why's that? Good question. I'll have a think about it and try to report back."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
